FROM eclipse-temurin:11-jdk-focal

ENV SPARK_VERSION=3.4.1
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip
ENV PYSPARK_PYTHON=python3
ENV JAVA_HOME=/opt/java/openjdk

# Install required packages
RUN apt-get update && \
    apt-get install -y wget python3 python3-pip && \
    rm -rf /var/lib/apt/lists/*

# Download and install Spark
RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Download Iceberg Spark Runtime JAR
RUN wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.2/iceberg-spark-runtime-3.4_2.12-1.4.2.jar \
    -O ${SPARK_HOME}/jars/iceberg-spark-runtime.jar

# Install AWS SDK bundle for S3
RUN wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar \
    -O ${SPARK_HOME}/jars/hadoop-aws-3.3.4.jar && \
    wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.261/aws-java-sdk-bundle-1.12.261.jar \
    -O ${SPARK_HOME}/jars/aws-java-sdk-bundle-1.12.261.jar

# Install Python dependencies
RUN pip3 install --no-cache-dir \
    pyspark==${SPARK_VERSION} \
    pandas \
    pyarrow

# Create workspace directory
WORKDIR /workspace

# Copy spark defaults configuration
COPY spark-defaults.conf ${SPARK_HOME}/conf/

# Keep container running
ENTRYPOINT ["tail", "-f", "/dev/null"]
